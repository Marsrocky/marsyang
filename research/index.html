<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Research - Jianfei Yang (Rocky)</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Research" />
<meta property="og:description" content="Research Interests My research mainly focuses on Physical AI, where AI enables physical systems—such as robotics, IoT, and embodied agents—to perceive, reason, and interact with the real world. Broadly speaking, I study how AI models enhance robotic perception and decision-making with multimodal sensor inputs (i.e. Multimodal AI, LLM, and VLA), how robots can personalize their behavior by understanding human states and interactions (i.e. Human-Centered Embodied AI), how foundation models can be efficiently deployed on edge devices to support real-time robotic perception and control (i." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://marsyang.site/research/" /><meta property="article:section" content="" />



	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/mystyle.css">

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Jianfei Yang (Mars)" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Jianfei Yang (Mars)</div>
					<div class="logo__tagline">Science is the only way to magic.</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">
				
				<span class="menu__text">Home</span>
				
			</a>
		</li>
		<li class="menu__item menu__item--active">
			<a class="menu__link" href="/research/">
				
				<span class="menu__text">Research</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/publications/">
				
				<span class="menu__text">Publications</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/award/">
				
				<span class="menu__text">Awards</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/group/">
				
				<span class="menu__text">Group</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/services/">
				
				<span class="menu__text">Services</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/patents/">
				
				<span class="menu__text">Patents</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/friends/">
				
				<span class="menu__text">Friends</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>


		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Research</h1>
			
		</header>
		<div class="content post__content clearfix">
			<hr>
<h3 id="research-interests">Research Interests</h3>
<p>My research mainly focuses on <strong>Physical AI</strong>, where AI enables physical systems—such as robotics, IoT, and embodied agents—to perceive, reason, and interact with the real world. Broadly speaking, I study how AI models enhance robotic perception and decision-making with multimodal sensor inputs (i.e. <strong>Multimodal AI, LLM, and VLA</strong>), how robots can personalize their behavior by understanding human states and interactions (i.e. <strong>Human-Centered Embodied AI</strong>), how foundation models can be efficiently deployed on edge devices to support real-time robotic perception and control (i.e. <strong>Edge Foundation Models for Robotics</strong>), and how robots generalize across embodiments and transfer knowledge from simulation to real-world tasks (i.e. <strong>Transfer Learning in Robotics</strong>). <u>My ultimate vision is to create embodied AI that seamlessly integrates perception, reasoning, and action, enabling robots to assist humans across diverse environments with adaptive intelligence and edge efficiency.</u></p>
<p>Revolving these goals, my team mainly studies the following topics:</p>
<ul>
<li>
<p><strong>Multimodal AI, LLM, and VLA Models</strong>: Traditional AI models predominantly rely on a single modality, limiting their adaptability and robustness. Our research focuses on developing multimodal foundation models that integrate diverse sensor inputs—such as vision, language, and action signals—to enhance perception, reasoning, and decision-making. By leveraging large language models (LLMs) and Vision-Language-Action (VLA) models, we enable robots and AI systems to understand and interact with the world more effectively, improving their ability to generalize across diverse environments and tasks.</p>
</li>
<li>
<p><strong>Human-Centered Embodied AI</strong>: Understanding human behavior is critical for advancing intelligent robotic systems. We leverage rich human perception data to improve human-robot interaction, prioritizing safety, adaptability, and intelligence. By modeling human behavior, pose dynamics, and cognitive states, we develop personalization strategies for embodied AI. These strategies ensure that robots can adapt their responses to individual users’ needs, preferences, and physical conditions, enhancing their autonomy and usability in real-world applications such as healthcare, assistive robotics, and industrial automation.</p>
</li>
<li>
<p><strong>Transfer Learning in Robotics</strong>: Developing robots that can generalize across different embodiments and environments is a fundamental challenge in robotics. Our research explores cross-embodiment transfer learning, enabling knowledge transfer between different robotic platforms, and sim-to-real transfer, bridging the gap between simulated training environments and real-world deployment. By leveraging domain adaptation, reinforcement learning, and self-supervised techniques, we aim to enhance the adaptability of robotic systems, reducing the need for extensive real-world data collection while improving their performance in unstructured and dynamic environments.</p>
</li>
<li>
<p><strong>Edge Foundation Models for Robotics</strong>: Deploying AI foundation models on resource-constrained edge devices is crucial for real-world robotic applications. Our research focuses on designing algorithms to minimize data and computational costs while preserving model accuracy. We develop efficient model compression, quantization, and optimization techniques to enable foundation models to run efficiently on edge hardware for robotic perception, planning, and control. Our ultimate goal is to create on-chip foundation models tailored for specific robotic tasks, ensuring scalable, low-latency, and energy-efficient AI inference.</p>
</li>
</ul>
<hr>
<!-- ### Research Projects
- **WiFi-based Sensing and Localization System**: We build a WiFi-based sensing system based on off-the-shelf WiFi routers and chips that can extract fine-grained channel state information for human sensing. Via WiFi signals, human poses and activities are recognized through walls, which enables security and healthcare applications. We have collabroated projects with *Panasonic* and *Singapore Airelines*.
- **Multimodal Learning and Systems**: We build the multimodal sensing system integrating radar, camera and WiFi in our lab. Multimodal learning algorithms are proposed for various sensing applications, such as human sensing or autonomous driving.
- **Data-Efficient Machine Learning**: To overcome the lack of data issue, we study transfer learning, domain adaptation and unsupervised learning for visual recognition and wireless sensing.

*** -->
<h3 id="collaborations">Collaborations</h3>
<p>I am interested in collaboration on the topics mentioned above. Apart from AI and IoT, I am also interested in AI-enabled interdisciplinary research, e.g., bioinformatics. Please drop me an email if you are interested in collaboration with a cool idea.</p>
<!-- I am interested in collaboration with respect to the following directions:
- IoT-enabled Human Perception and Its Applications (Smart Home and Robotics)
- Human-Centric AI via Computer Vision and IoT Sensors (Human-Centric AI Systems)
- Deep Learning and Transfer Learning Algorithms and Applications for Interdisciplinary Research (Biology and Medicine) -->
<!-- <center>
<img src="/img/WiFi-sensing.png" alt="wifi-human-sensing" width="500" height="600">
</center> -->
		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2025 Jianfei Yang (Rocky).
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>